[model]
# Path to folder where the model files will be stored.
data_folder = models
# Size of the GPT-2 model. Could be one of 'small' (117M), 'medium' (345M) or 'large' (1542M).
# Select small for CPU or experimentation, and medium for GPU
model_size = small
# Dataset name the model was trained on. One of 'multiref' (147M multi-turn dialogue 
# from Reddit discussion thread) or 'dstc' (DSTC-7 grounded dialogue generation challenge).
dataset = multiref
# True: load model trained from scratch or False: load model trained from fine-tuning the GPT-2.
from_scratch = False

[dialog]
# Seed for random number generators, fix seed to reproduce results.
seed
# Avoid using CUDA when available.
no_cuda = False
# The number of turns the model should consider. 
# Set to 0 to focus on the last message. Set to -1 to disable.
turns_memory = 2
# Float value controlling randomness in boltzmann
# distribution. Lower temperature results in less random completions. As the
# temperature approaches zero, the model will become deterministic and
# repetitive. Higher temperature results in more random completions.
temperature = 0.7
# Integer value controlling diversity. 1 means only 1 word is
# considered for each step (token), resulting in deterministic completions,
# while 40 means 40 words are considered at each step. 0 (default) is a
# special setting meaning no restrictions. 40 generally is a good value.
top_k = 40
# Like top_k, top_p is a constraint on the craziness of the output
top_p = 0.0
# Length of text to be returned, inclusive of punctuations etc.
length = 32
# Number of samples to return
num_samples = 1

[telegram]
# Your Telegram token. See https://core.telegram.org/bots
token = 956127710:AAHRQqzITV-MWPMOd6xQuNfWK807DIO_XAk
